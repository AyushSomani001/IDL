{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure Google Colaboratory (if needed)\n",
    "Set the variable `colab` to `True` if you are using this notebook from Google Colaboratory, else set it to `False`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colab = False  # Set to true if working on Google Colaboratory\n",
    "\n",
    "if colab:\n",
    "    # Mount your Google Drive for storage\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    \n",
    "    # Save to, and access files from, your Google Drive if using Colaboratory \n",
    "    ROOT_DIR = '/content/drive/MyDrive/IDL'\n",
    "    \n",
    "    # Fetch the git repository\n",
    "    !git clone https://github.com/AyushSomani001/IDL {ROOT_DIR}\n",
    "\n",
    "else:\n",
    "     ROOT_DIR = '..'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5Cvs94Cf77x-"
   },
   "outputs": [],
   "source": [
    "reqs_file = f'{ROOT_DIR}/requirements.txt'\n",
    "!pip install -r {reqs_file}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4w3Jk-qWembG"
   },
   "source": [
    "### Summary\n",
    "The code present in this notebook provides a tutorial on using Grad-CAM (Gradient-weighted Class Activation Mapping) to visualize and interpret the activations of a CNN (Convolutional Neural Network) model. The tutorial demonstrates how to use Grad-CAM for object localization and fine-tuning the model to improve its accuracy.\n",
    "\n",
    "The tutorial uses `PyTorch`, a popular deep learning framework, and provides step-by-step instructions on implementing Grad-CAM on a pre-trained CNN model. It also includes code for computing CAM (Class Activation Mapping) metrics, which can be used to evaluate the quality of the Grad-CAM visualizations.\n",
    "\n",
    "Additionally, the tutorial provides guidance on tuning hyperparameters such as learning rate, weight decay, and momentum to improve model performance. It also includes code for visualizing the feature maps and gradients of the CNN model, which can aid in understanding how the model is making predictions. Overall, the tutorial is a comprehensive guide to using Grad-CAM for model interpretation and improvement.\n",
    "\n",
    "<hr>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uBRCvwWUfa93"
   },
   "source": [
    "One of the main focuses of the tutorial is on exploring the different metrics that can be used to compare and evaluate pixel attribution techniques.\n",
    "\n",
    "The tutorial explains the importance of measuring the quality of model explanations and provides insight into the kind of problems that arise when using metrics for model explanations. Moreover, the tutorial demonstrates how we can use these metrics to fine-tune the model explanations and improve the model's overall performance.\n",
    "\n",
    " \n",
    "\n",
    "1.   The tutorial uses the pytorch-grad-cam package and provides a step-by-step guide to extracting the different metrics available in the package.\n",
    "2.  Includes examples of how to use these metrics to improve model explanations for images.\n",
    "\n",
    "Overall, this tutorial serves as a valuable resource for anyone looking to benchmark and tune model explanations. By understanding the different metrics and their practical applications, readers can gain insights into the inner workings of their models and improve their performance.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 403,
     "status": "ok",
     "timestamp": 1683144345462,
     "user": {
      "displayName": "Ayush Somani",
      "userId": "03363251164158417249"
     },
     "user_tz": -120
    },
    "id": "V30T-AX4icFc",
    "outputId": "1a0ba508-a838-4b72-c3b9-4c84e22bd164"
   },
   "outputs": [],
   "source": [
    "%cd {ROOT_DIR}/Notebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 503,
     "status": "ok",
     "timestamp": 1683144350932,
     "user": {
      "displayName": "Ayush Somani",
      "userId": "03363251164158417249"
     },
     "user_tz": -120
    },
    "id": "HuUdulnIwzJU"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.extend([f'{ROOT_DIR}/Notebooks', f'{ROOT_DIR}/Notebooks/Toolkit/utils', f'{ROOT_DIR}/Notebooks/library'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sQNLboV0QLMU"
   },
   "source": [
    "In the GradCAM++ paper image above the masked images are black, so it looks like they are multiplying the original image, before the image-net normalization.\n",
    "\n",
    "I'm pretty sure that most of the methods that came after that, multiplied the tensor in the input to the model, after image-net normalization.\n",
    "\n",
    "Both should be similar, but with difference that maybe black pixels (multiplying pre normalization) make the images look more different than the distribution of natural images, and maybe that has some effect.\n",
    "\n",
    "No one is sharing the CAM evaluation code, and no one is commenting about that implementation detail in the papers, so I'm pretty sure the different papers are using differnt metrics ;-)\n",
    "\n",
    "Before moving on to other metrics, lets take a first look at how to use these metrics in the pytorch-grad-cam package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 241
    },
    "executionInfo": {
     "elapsed": 2577,
     "status": "ok",
     "timestamp": 1683142219430,
     "user": {
      "displayName": "Ayush Somani",
      "userId": "03363251164158417249"
     },
     "user_tz": -120
    },
    "id": "rIUv1GXAelj0",
    "outputId": "37c63cd2-fd7c-4db2-d468-b9deb9a32421"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from torchvision import models\n",
    "import numpy as np\n",
    "import cv2\n",
    "import requests\n",
    "\n",
    "from grad_cam import GradCAM\n",
    "from model_targets import ClassifierOutputTarget\n",
    "from image import show_cam_on_image, deprocess_image, preprocess_image\n",
    "from PIL import Image\n",
    "\n",
    "model = models.resnet50(pretrained=True)\n",
    "model.eval()\n",
    "\n",
    "# # Fetch an image from an URL\n",
    "# image_url = \"https://abc.com/th/id/R=0\"\n",
    "# img = np.array(Image.open(requests.get(image_url, stream=True).raw))\n",
    "\n",
    "# Set the path to the image file\n",
    "image_path = f'{ROOT_DIR}/Notebooks/Sample_data/Thoughful_monkey.jpg'\n",
    "\n",
    "# Load the image using PIL library\n",
    "img = np.array(Image.open(image_path))\n",
    "\n",
    "#Preprocess the image for the model\n",
    "img = cv2.resize(img, (224, 224))\n",
    "img = np.float32(img) / 255\n",
    "input_tensor = preprocess_image(img, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "\n",
    "# The target for the CAM is the monkey category \n",
    "#(Check your output class here: https://deeplearning.cms.waikato.ac.nz/user-guide/class-maps/IMAGENET/).\n",
    "# As usual for classication, the target is the logit output\n",
    "# before softmax, for that category.\n",
    "targets = [ClassifierOutputTarget(368)]\n",
    "target_layers = [model.layer4]\n",
    "with GradCAM(model=model, target_layers=target_layers) as cam:\n",
    "    grayscale_cams = cam(input_tensor=input_tensor, targets=targets)\n",
    "    cam_image = show_cam_on_image(img, grayscale_cams[0, :], use_rgb=True)\n",
    "cam = np.uint8(255*grayscale_cams[0, :])\n",
    "cam = cv2.merge([cam, cam, cam])\n",
    "images = np.hstack((np.uint8(255*img), cam , cam_image))\n",
    "Image.fromarray(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 277
    },
    "executionInfo": {
     "elapsed": 2238,
     "status": "ok",
     "timestamp": 1683142363198,
     "user": {
      "displayName": "Ayush Somani",
      "userId": "03363251164158417249"
     },
     "user_tz": -120
    },
    "id": "LVmppGNoo7Si",
    "outputId": "8298271a-a0b3-453f-911f-561cd4336c53"
   },
   "outputs": [],
   "source": [
    "# Now lets see how to evaluate this explanation:\n",
    "from cam_mult_image import CamMultImageConfidenceChange\n",
    "from model_targets import ClassifierOutputSoftmaxTarget\n",
    "\n",
    "# For the metrics we want to measure the change in the confidence, after softmax, that's why\n",
    "# we use ClassifierOutputSoftmaxTarget.\n",
    "targets = [ClassifierOutputSoftmaxTarget(368)]\n",
    "cam_metric = CamMultImageConfidenceChange()\n",
    "scores, visualizations = cam_metric(input_tensor, grayscale_cams, targets, model, return_visualization=True)\n",
    "score = scores[0]\n",
    "visualization = visualizations[0].cpu().numpy().transpose((1, 2, 0))\n",
    "visualization = deprocess_image(visualization)\n",
    "print(f\"The confidence increase percent: {100*score}\")\n",
    "print(\"The visualization of the pertubated image for the metric:\")\n",
    "Image.fromarray(visualization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8z4Y24n7Pvgt"
   },
   "source": [
    "The confidence increase here is positive.\n",
    "\n",
    "That's a good sign - the CAM reduced noise from other parts of the image and retains the information that triggers the category output.\n",
    "\n",
    "1. The \"drop in confidence\" metric here from the gradcam++ paper would be 0 (since it's negative).\n",
    "\n",
    "2. The \"increase in confidence\" metric would be 1 (since there is an increase in confidence).\n",
    "\n",
    "For completeness, lets see how we can use those metrics directly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 701,
     "status": "ok",
     "timestamp": 1683142399991,
     "user": {
      "displayName": "Ayush Somani",
      "userId": "03363251164158417249"
     },
     "user_tz": -120
    },
    "id": "Kn1t7FQNp2fT",
    "outputId": "e6ac8598-4930-4b9f-d0ac-eb2439b91e9b"
   },
   "outputs": [],
   "source": [
    "from cam_mult_image import DropInConfidence, IncreaseInConfidence\n",
    "print(\"Drop in confidence\", DropInConfidence()(input_tensor, grayscale_cams, targets, model))\n",
    "print(\"Increase in confidence\", IncreaseInConfidence()(input_tensor, grayscale_cams, targets, model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9_fcQyKQPrBv"
   },
   "source": [
    "Similarly, we could pertubate the image by deleting pixels with high values in the CAM. In this case, we would WANT a larger drop in the confidence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 277
    },
    "executionInfo": {
     "elapsed": 861,
     "status": "ok",
     "timestamp": 1683142407582,
     "user": {
      "displayName": "Ayush Somani",
      "userId": "03363251164158417249"
     },
     "user_tz": -120
    },
    "id": "ZHyi5agvqDnr",
    "outputId": "36e7e857-dd9a-4ef5-f8e6-3c2e9a9ea2fa"
   },
   "outputs": [],
   "source": [
    "inverse_cams = 1 - grayscale_cams\n",
    "scores, visualizations = CamMultImageConfidenceChange()(input_tensor, inverse_cams, targets, model, return_visualization=True)\n",
    "score = scores[0]\n",
    "visualization = visualizations[0].cpu().numpy().transpose((1, 2, 0))\n",
    "visualization = deprocess_image(visualization)\n",
    "print(f\"The confidence increase percent: {score}\")\n",
    "print(\"The visualization of the pertubated image for the metric:\")\n",
    "Image.fromarray(visualization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G8gBRwoBqR1D"
   },
   "source": [
    "### We deleted important parts, how come it became more confident ?]\n",
    "When we pertubate the supposedly impotant parts, the model actually becomes more confident than before. Maybe the CAM explanation wasn't telling us the whole story in the first place, and there are other parts that were important as well that is was missing. This shows us the importance of using complementory metrics, and how these metrics can contradict each other.\n",
    "\n",
    "Another problem here however is that the CAM itself has values that are medium range all over it: after the pertubation, we can still see the bear in the image. Getting the Image*CAM to work requires the CAM to have a distribution that's more stretched.\n",
    "\n",
    "This is the motivation for other methods that threshold the CAM and create binary masks. Lets completely remove the highest scoring 25%, and see that the model confidence drops now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 277
    },
    "executionInfo": {
     "elapsed": 785,
     "status": "ok",
     "timestamp": 1683142415894,
     "user": {
      "displayName": "Ayush Somani",
      "userId": "03363251164158417249"
     },
     "user_tz": -120
    },
    "id": "wtE-p_jkqRZM",
    "outputId": "0956f895-a4b1-46fc-cb69-521b34249257"
   },
   "outputs": [],
   "source": [
    "thresholded_cam = grayscale_cams < np.percentile(grayscale_cams, 75)\n",
    "scores, visualizations = CamMultImageConfidenceChange()(input_tensor, thresholded_cam, targets, model, return_visualization=True)\n",
    "score = scores[0]\n",
    "visualization = visualizations[0].cpu().numpy().transpose((1, 2, 0))\n",
    "visualization = deprocess_image(visualization)\n",
    "print(f\"The confidence increase: {score}\")\n",
    "print(\"The visualization of the pertubated image for the metric:\")\n",
    "Image.fromarray(visualization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g6lzxCtwPHS0"
   },
   "source": [
    "But the image above doesn't look quite natural, does it ?\n",
    "\n",
    "### Remove and Retrain (ROAR)\n",
    "https://proceedings.neurips.cc/paper/2019/hash/fe4b8556000d0f0cae99daa5c5c5a410-Abstract.html\n",
    "\n",
    "When you pertubate the image by modifying the highest scoring regions, and the confidence decreases:\n",
    "\n",
    "*   Is it because the explanation is good and all of cues the model was using were now removed ?\n",
    "*   Or is it because of the pertubation method itself that's causing a distribution shift in the data: maybe the new image is so different than what the model expects, so un-natural, that the confidence drops.\n",
    "\n",
    "Maybe it thinks that all that gray above is a road, or an airplane.\n",
    "\n",
    "The claim in this paper is that the high confidence drops the different methods show, is actually because of the latter. When they pertubate the images by removing highest scoring pixels and then retrain, they the model is still actually much more accurate than expected. Because of this, they argue that we should retrain on the pertubated images to be able to adapt to the pertubations. If the explanation method still scores high - we know we can trust it much more.\n",
    "\n",
    "They also show that common methods (altough they focused more on gradient based method and not on CAM methods) are worse on this benchmark then classical computer vision edge detector (Sobel) that doesn't depend on the model parameters at all. So basically some methods that claim they are better than others, aren't really.\n",
    "\n",
    "Benchmarking against random explanations, or explanations that don't depend on the model (like edge detection), is a very interesting idea in itself, so lets take a look at that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 277
    },
    "executionInfo": {
     "elapsed": 2171,
     "status": "ok",
     "timestamp": 1683142425497,
     "user": {
      "displayName": "Ayush Somani",
      "userId": "03363251164158417249"
     },
     "user_tz": -120
    },
    "id": "VHUkGqGwqWkr",
    "outputId": "91fac173-3e92-4fdf-8677-443b26f474a5"
   },
   "outputs": [],
   "source": [
    "from sobel_cam import sobel_cam\n",
    "\n",
    "sobel_cam_grayscale = sobel_cam(np.uint8(img * 255))\n",
    "thresholded_cam = sobel_cam_grayscale < np.percentile(sobel_cam_grayscale, 75)\n",
    "\n",
    "cam_metric = CamMultImageConfidenceChange()\n",
    "scores, visualizations = cam_metric(input_tensor, [thresholded_cam], targets, model, return_visualization=True)\n",
    "score = scores[0]\n",
    "visualization = visualizations[0].cpu().numpy().transpose((1, 2, 0))\n",
    "visualization = deprocess_image(visualization)\n",
    "print(f\"The confidence increase: {score}\")\n",
    "print(\"The visualization of the pertubated image for the metric:\")\n",
    "sobel_cam_rgb = cv2.merge([sobel_cam_grayscale, sobel_cam_grayscale, sobel_cam_grayscale])\n",
    "Image.fromarray(np.hstack((sobel_cam_rgb, visualization)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l-vZUWiZPAvi"
   },
   "source": [
    "Ok, good. So our CAM scores higher than a classic edge detector on this benchmark. That's good news !\n",
    "\n",
    "Back to the ROAR method - Re-training a model on the pertubated images is very expensive. We don't even always know in advance what explanation method we want to chose. For many users this won't be a practical approach. Altough if the stakes are high and you want to be 100% sure about the explanation, this is something to consider.\n",
    "\n",
    "So what can we do?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J9SX8sbqOxZp"
   },
   "source": [
    "### Sanity Checks for Saliency Metrics\n",
    "https://ojs.aaai.org/index.php/AAAI/article/view/6064\n",
    "\n",
    "This paper introduces a metric called \"Area Over the Perturbation Curve\" - AOPC, which is the average confidence drop over different removal percentiles. (We will later call that MostRelevantFirstAverage or LeastRelevantFirstAverage since I think it's a bit more explicit and clear, but it's the same).\n",
    "\n",
    "They check different imputation strategies:\n",
    "\n",
    "Replacing pixels that need to be deleted, by random values.\n",
    "Replacing pixels that need to be deleted, by the mean (for R,G,B separately) in the image.\n",
    "Removing the highest attention pixels first: Most Relevant First (MORF).\n",
    "Removing the least attention pixels first: Least Relevant First (LERF).\n",
    "And then check how different algorithms differ on average acrross a dataset, but also for different individual images. Ultimately we care about an individual image - we want to make sure the explanation we use for it is reliable.\n",
    "\n",
    "The conclusion (in my own words) is that it's a wild west. Different imputation strategies give different results. MORF and LERF give different results for different algorithms, and basically measure different properties. For the same image, it's difficult to know in advance what explanation strategy will work best.\n",
    "\n",
    "This means that going forward, we will need a combination of metrics, will need take in mind the imputation strategy, and will definately need to look at every image individually."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XrMG5zuVqt8U"
   },
   "source": [
    "### ROAD: Remove and Debias\n",
    "https://proceedings.mlr.press/v162/rong22a.html\n",
    "\n",
    "The claim in this paper is that the perbutbated image (they use the term \"imputed image\") itself is leaking data. Even the previous ROAR method can suffer from that. They use a beutiful example so I'm going to just paste that here:\n",
    "\n",
    "Imagine a two-class problem that consists of detecting whether an object is located on the left or the right side of an image. A reasonable attribution method masks out pixels on the left or the right depending on the location of the object. In this case, the retraining step can lead to a classifier that infers the class just from the location of the masked out pixels and obtain high accuracy.\n",
    "\n",
    "They further show that it's easy to train models that predict what pixels are a result of the pertubation, and that it's possible to train models with surprising accuracy using just the binary masks. So it's possible to detect the mask, and then infer things from it.\n",
    "\n",
    "To solve this they propose a pertubation method that's more difficult to detect. And since it's good and there is less of a distribution shift, training with ROAR doesn't have an advantage any more: different metrics are more consistent with each other.\n",
    "\n",
    "What they do is replace every pixel that needs to be removed with a weighted average of it's neighbours. Since some of it's neighbours might also need to be removed, we get system of linear equations that we have to solve, to find the new values of the pixels we want to replace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "executionInfo": {
     "elapsed": 3910,
     "status": "ok",
     "timestamp": 1683142446241,
     "user": {
      "displayName": "Ayush Somani",
      "userId": "03363251164158417249"
     },
     "user_tz": -120
    },
    "id": "pDNNi1uZquXW",
    "outputId": "aa81a126-7836-45ab-c7d3-879d1247bb21"
   },
   "outputs": [],
   "source": [
    "from road import ROADMostRelevantFirst\n",
    "cam_metric = ROADMostRelevantFirst(percentile=75)\n",
    "scores, visualizations = cam_metric(input_tensor, grayscale_cams, targets, model, return_visualization=True)\n",
    "score = scores[0]\n",
    "visualization = visualizations[0].cpu().numpy().transpose((1, 2, 0))\n",
    "visualization = deprocess_image(visualization)\n",
    "print(f\"The confidence increase when removing 25% of the image: {score}\")\n",
    "\n",
    "cam_metric = ROADMostRelevantFirst(percentile=90)\n",
    "scores, visualizations = cam_metric(input_tensor, grayscale_cams, targets, model, return_visualization=True)\n",
    "score = scores[0]\n",
    "visualization_10 = visualizations[0].cpu().numpy().transpose((1, 2, 0))\n",
    "visualization_10 = deprocess_image(visualization_10)\n",
    "print(f\"The confidence increase when removing 10% of the image: {score}\")\n",
    "print(\"The visualizations:\")\n",
    "Image.fromarray(np.hstack((visualization, visualization_10)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3e4oUJyIN3a8"
   },
   "source": [
    "This is much better than replacing with the gray. But to be honest it's still quite distinguishable from the rest of the image since it's so blurry. So more work is needed to make this a convincing pertubation. GANs can be useful here (and were used in the literature for this), but are computationally expensive.\n",
    "\n",
    "How much from the image should we remove ?\n",
    "\n",
    "That depends on the object size and varies, so it makes sense to try different percentiles and then take the average, if we want a more robust metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 240
    },
    "executionInfo": {
     "elapsed": 6040,
     "status": "error",
     "timestamp": 1683144365359,
     "user": {
      "displayName": "Ayush Somani",
      "userId": "03363251164158417249"
     },
     "user_tz": -120
    },
    "id": "3s0pPLIzN7qY",
    "outputId": "a97d666d-1087-421d-f43b-5a55d3364f17"
   },
   "outputs": [],
   "source": [
    "from road import ROADMostRelevantFirstAverage\n",
    "cam_metric = ROADMostRelevantFirstAverage(percentiles=[20, 40, 60, 80])\n",
    "scores = cam_metric(input_tensor, grayscale_cams, targets, model)\n",
    "print(f\"The average confidence increase with ROAD accross 4 thresholds: {scores[0]}\")\n",
    "scores = cam_metric(input_tensor, [sobel_cam_grayscale], targets, model)\n",
    "print(f\"The average confidence increase for Sobel edge detection with ROAD accross 4 thresholds: {scores[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8NMtc3MsNPWu"
   },
   "source": [
    "So we have ethods that can rank our model explanations. We saw we can compare that against a sobel edge detector as a sanity check to see we're better.\n",
    "\n",
    "Lets see how to use this to get better explanations.\n",
    "\n",
    "We will also use a toy RandomCAM that generates CAMs with random uniform values in the range [-1, 1] for the spatial activations. If our CAM methods are that smart, they should be much better than it, on average."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 238
    },
    "executionInfo": {
     "elapsed": 32731,
     "status": "ok",
     "timestamp": 1683142641745,
     "user": {
      "displayName": "Ayush Somani",
      "userId": "03363251164158417249"
     },
     "user_tz": -120
    },
    "id": "VTQq7LQ1rcvk",
    "outputId": "88de43d7-e744-40fb-c5e0-4cb178ff78e6"
   },
   "outputs": [],
   "source": [
    "from grad_cam import GradCAMPlusPlus\n",
    "from eigen_cam import EigenGradCAM\n",
    "from ablation_cam import AblationCAM\n",
    "from random_cam import RandomCAM\n",
    "from road import ROADCombined\n",
    "\n",
    "# Showing the metrics on top of the CAM : \n",
    "def visualize_score(visualization, score, name, percentiles):\n",
    "    visualization = cv2.putText(visualization, name, (10, 20), \n",
    "                                cv2.FONT_HERSHEY_SIMPLEX, 0.4, (255,255,255), 1, cv2.LINE_AA)\n",
    "    visualization = cv2.putText(visualization, \"(Least first - Most first)/2\", (10, 40), \n",
    "                                cv2.FONT_HERSHEY_SIMPLEX, 0.3, (255,255,255), 1, cv2.LINE_AA)\n",
    "    visualization = cv2.putText(visualization, f\"Percentiles: {percentiles}\", (10, 55), \n",
    "                                cv2.FONT_HERSHEY_SIMPLEX, 0.4, (255,255,255), 1, cv2.LINE_AA)    \n",
    "    visualization = cv2.putText(visualization, \"Remove and Debias\", (10, 70), \n",
    "                                cv2.FONT_HERSHEY_SIMPLEX, 0.4, (255,255,255), 1, cv2.LINE_AA) \n",
    "    visualization = cv2.putText(visualization, f\"{score:.5f}\", (10, 85), \n",
    "                                cv2.FONT_HERSHEY_SIMPLEX, 0.4, (255,255,255), 1, cv2.LINE_AA)    \n",
    "    return visualization\n",
    "    \n",
    "def benchmark(input_tensor, target_layers, eigen_smooth=False, aug_smooth=False, category=281):\n",
    "    methods = [(\"GradCAM\", GradCAM(model=model, target_layers=target_layers, use_cuda=True)),\n",
    "               (\"GradCAM++\", GradCAMPlusPlus(model=model, target_layers=target_layers, use_cuda=True)),\n",
    "               (\"EigenGradCAM\", EigenGradCAM(model=model, target_layers=target_layers, use_cuda=True)),\n",
    "               (\"AblationCAM\", AblationCAM(model=model, target_layers=target_layers, use_cuda=True)),\n",
    "               (\"RandomCAM\", RandomCAM(model=model, target_layers=target_layers, use_cuda=True))]\n",
    "\n",
    "    cam_metric = ROADCombined(percentiles=[20, 40, 60, 80])\n",
    "    targets = [ClassifierOutputTarget(category)]\n",
    "    metric_targets = [ClassifierOutputSoftmaxTarget(category)]\n",
    "    \n",
    "    visualizations = []\n",
    "    percentiles = [10, 50, 90]\n",
    "    for name, cam_method in methods:\n",
    "        with cam_method:\n",
    "            attributions = cam_method(input_tensor=input_tensor, \n",
    "                                      targets=targets, eigen_smooth=eigen_smooth, aug_smooth=aug_smooth)\n",
    "        attribution = attributions[0, :]    \n",
    "        scores = cam_metric(input_tensor, attributions, metric_targets, model)\n",
    "        score = scores[0]\n",
    "        visualization = show_cam_on_image(cat_and_dog, attribution, use_rgb=True)\n",
    "        visualization = visualize_score(visualization, score, name, percentiles)\n",
    "        visualizations.append(visualization)\n",
    "    return Image.fromarray(np.hstack(visualizations))\n",
    "\n",
    "cat_and_dog_image_url = \"https://raw.githubusercontent.com/jacobgil/pytorch-grad-cam/master/examples/both.png\"\n",
    "cat_and_dog = np.array(Image.open(requests.get(cat_and_dog_image_url, stream=True).raw))\n",
    "\n",
    "#cat_and_dog_image = '/content/drive/MyDrive/Notebook/data/cat_and_dog.jpg'\n",
    "#cat_and_dog = np.array(Image.open(cat_and_dog_image))\n",
    "\n",
    "cat_and_dog = np.float32(cat_and_dog) / 255\n",
    "input_tensor = preprocess_image(cat_and_dog, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "target_layers = [model.layer4]\n",
    "\n",
    "model.cuda()\n",
    "input_tensor = input_tensor.cuda()\n",
    "np.random.seed(42)\n",
    "benchmark(input_tensor, target_layers, eigen_smooth=False, aug_smooth=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7fo5oiz2NBMs"
   },
   "source": [
    "So EigenGradCAM is a clear winner, not unsurprisingly since it also looks smoother.\n",
    "\n",
    "What about if we use an earlier layer with lower level features?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 238
    },
    "executionInfo": {
     "elapsed": 41057,
     "status": "ok",
     "timestamp": 1683143968586,
     "user": {
      "displayName": "Ayush Somani",
      "userId": "03363251164158417249"
     },
     "user_tz": -120
    },
    "id": "9vP7S2hsMgrx",
    "outputId": "76c90991-9928-4ab5-9537-4dde8e4fb5c7"
   },
   "outputs": [],
   "source": [
    "target_layers = [model.layer4[-2]]\n",
    "benchmark(input_tensor, target_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 238
    },
    "executionInfo": {
     "elapsed": 36603,
     "status": "ok",
     "timestamp": 1683144008848,
     "user": {
      "displayName": "Ayush Somani",
      "userId": "03363251164158417249"
     },
     "user_tz": -120
    },
    "id": "qVI4AnsbMmJf",
    "outputId": "f04e654b-9ac2-4077-d054-117d7a9d08ce"
   },
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "benchmark(input_tensor, target_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2LJkG8JtMouF",
    "outputId": "2336ed3a-38ce-4507-8996-eb0530254d22"
   },
   "outputs": [],
   "source": [
    "# Let's look how it looks for one of the dog categories (that the model is much less confident about)\n",
    "np.random.seed(0)\n",
    "benchmark(input_tensor, target_layers, category=246)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KkoeApdhMsTP"
   },
   "source": [
    "So using this metric we can see that:\n",
    "\n",
    "1. The different methods perform quite different.\n",
    "2. You can use the metric to tune parameters like which layer or explainability method to use.\n",
    "3. The explanations are quite different between the different methods.\n",
    "4. A Random CAM isn't always that behind some of the methods, but still they are much better than random.\n",
    "\n",
    "In any case we need to be suspicious about the results and double check them against benchmarks like RandomCAM or Sobel, before extracting too many insights from them."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyNpfSLezv/FDdTfO4NOQqCR",
   "mount_file_id": "1pD8KPWrMsVscVcZ3PUcgTZ34EnE1e37U",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
