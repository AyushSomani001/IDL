# [Interpretability in Deep Learning](https://link.springer.com/book/10.1007/978-3-031-20639-9)
### Authors: Ayush Somani, Alexander Horsch, Dilip K.Prasad
This is a repository that contains practice module for the monograph 'Interpretability in Deep Learning'. This is recommended to AI practitioners and basically anyone who wants an overview of techniques to make their deep learning models more interpretable. To benefit from this book considerably, we expect the readers to be acquainted with basic deep learning terminologies. However, we attempt to introduce fundamental concepts for the convenience of the readers in the book. It should be likely to understand the intuitive explanation of the technique at the beginning of each chapter without mathematics.


# Research Papers - To Read


| Paper Title   |   Conference/Journal |   Author  | Description    |   
|:----------|:----------|:------|:------|
| [Towards Robust Interpretability with Self-Explaining Neural Networks!](https://proceedings.neurips.cc/paper_files/paper/2018/file/3e9f0fc9b2f89e043bc6233994dfcf76-Paper.pdf) | NeurIPS 2018 | D. Alvarez-Melis and T.S. Jaakkola | Proposes self-explaining neural networks, which learn to provide interpretable explanations for their predictions by incorporating explicit explanatory factors into the model architecture. | 
| [Sanity Checks for Saliency Maps](https://proceedings.neurips.cc/paper_files/paper/2018/file/294a8ed24b1ad22ec2e7efea049b8737-Paper.pdf) | NeurIPS 2018 | Adebayo et al. | Highlights the limitations of saliency methods and proposes sanity checks to ensure that the explanations provided by these methods are meaningful and reliable.|
| [On the (In)fidelity and Sensitivity of Explanations](https://proceedings.neurips.cc/paper_files/paper/2019/file/a7471fdc77b3435276507cc8f2dc2569-Paper.pdf) | NeurIPS 2019 | Yeh at al. | Introduces two metrics, fidelity and sensitivity, to evaluate the quality of explanations provided by various interpretability methods, including saliency and CAMs.|
|[Invariant Risk Minimization](https://arxiv.org/abs/1907.02893) | arXiv 2019| Arjovsky et al. | Introduces Invariant Risk Minimization, a learning framework that encourages models to rely on features that are invariant across different environments, leading to more robust and interpretable predictions|
| [Explanation by Progressive Exaggeration](https://openreview.net/forum?id=H1xFWgrFPS) | ICLR 2020 | Singla et al. | A new method which iteratively exaggerates the most important features in the input to generate more robust and interpretable explanations. [GitHub](https://github.com/batmanlab/Explanation_by_Progressive_Exaggeration)|
| [Relevance-CAM: Your Model Already Knows Where to Look!](https://openaccess.thecvf.com/content/CVPR2021/papers/Lee_Relevance-CAM_Your_Model_Already_Knows_Where_To_Look_CVPR_2021_paper.pdf) | CVPR 2021 | Lee et al.  |  | 
| [Neural Prototype Trees for Interpretable Fine-Grained Image Recognition](https://openaccess.thecvf.com/content/CVPR2021/papers/Nauta_Neural_Prototype_Trees_for_Interpretable_Fine-Grained_Image_Recognition_CVPR_2021_paper.pdf) | CVPR 2021 | Nauta et al. |  |  
| [Concept-Monitor: Understanding DNN training through individual neurons](https://arxiv.org/pdf/2304.13346.pdf)| arXiv April 2023 | Khan et al.| |

<details open>
  <summary><strong>Recent and Interesting Archived Papers on Chat-GPT for Research:</strong></summary>
     [1] <a href="https://arxiv.org/pdf/2304.11567.pdf">Differentiate ChatGPT-generated and Human-written Medical Texts</a> | Liao et al. (2023)
    <br/>
    [2] <a href="https://arxiv.org/pdf/2304.08979.pdf">In ChatGPT We Trust? Measuring and Characterizing the Reliability of ChatGPT</a> | Shen et al. (2023)
    <br/>
    [3] <a href="https://arxiv.org/pdf/2304.05335.pdf"> Toxicity in CHATGPT: Analyzing Persona-assigned Language Models </a> | Deshpande et al. (2023)
   <br/> 
</details>


<details open>
  <summary><strong>Intriguing Model Improvement Application Paper:</strong></summary>
     [1] <a href="https://openaccess.thecvf.com/content_CVPR_2019/papers/Choe_Attention-Based_Dropout_Layer_for_Weakly_Supervised_Object_Localization_CVPR_2019_paper.pdf">Attention-based Dropout Layer for Weakly Supervised Object Localization</a> | Junsuk Choe, Hyunjung Shim (CVPR 2019) | <img src="https://edent.github.io/SuperTinyIcons/images/svg/github.svg" width="16" /> <a href="https://github.com/junsukchoe/ADL">GitHub</a> |
    <br/>
</details>

# Citation
If you wish to cite the book "Interpretability in Deep Learning", feel free to use this [BibTeX](http://www.bibtex.org/) reference:

```bibtex
@book{somani2023interpretability,
  title={Interpretability in Deep Learning},
  author={Somani, Ayush and Horsch, Alexander and Prasad, Dilip K},
  year={2023},
  publisher={Springer Nature}
}
```

## Contributing

Feeling like extending the range of possibilities of interpretable methods to make AI more trustable? Or perhaps submitting a paper implementation? Any sort of contribution is greatly appreciated!
